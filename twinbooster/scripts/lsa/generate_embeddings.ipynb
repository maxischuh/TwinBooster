{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from twinbooster import LSA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm.auto as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.DataFrame()\n",
    "\n",
    "# Open and concatenate all pkl text files to a list of strings\n",
    "path = \"../llm_dataset/text/\"\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith(\".pkl\"):\n",
    "        chunk = pd.read_pickle(path + file)\n",
    "        corpus = pd.concat([corpus, chunk], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1242339 entries, 772172 to 744836\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count    Dtype \n",
      "---  ------  --------------    ----- \n",
      " 0   aid     1242339 non-null  object\n",
      " 1   text    1242339 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 28.4+ MB\n"
     ]
    }
   ],
   "source": [
    "corpus_train = corpus.sample(frac=.8, random_state=42)\n",
    "corpus_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "/workspace/tox/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/workspace/tox/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lsa = LSA()\n",
    "lsa.fit(corpus_train[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = lsa.encode(corpus[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"lsa_embeddings.npy\", embeddings)\n",
    "df = pd.DataFrame(columns=corpus[\"aid\"].astype(int), data=embeddings.T)\n",
    "df.to_pickle(\"lsa_embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 355 entries, 0 to 354\n",
      "Columns: 1552924 entries, 1 to 99999\n",
      "dtypes: float32(1552924)\n",
      "memory usage: 2.1 GB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tox",
   "language": "python",
   "name": "tox"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
